\chapter{Revisão Bibliográfica}
% Label para referenciar
\label{revisao}
Neste capítulo, os conceitos fulcrais para o entendimento e desenvolvimento do projeto são abordados e detalhados.

\section{Democracia} Pode-se observar a existência de dois tipos principais de democracia ao redor do globo. A Democracia Direta, também conhecida como Democracia Pura, na qual todos os cidadãos detêm o poder de participar diretamente do processo de tomada de decisão. Os modelos atuais de democracia seguem a vertente da Democracia Representativa, na qual indivíduos são eleitos para representar os interesses de determinado grupo no processo de tomada de decisão do Estado \cite{impactofictonreiforcingcitizens}. 

Alguns autores defendem que a Democracia Direta seria mais adequada em uma sociedade democrática considerando a efetividade de sua participação nos processos decisórios, porém este modelo apresenta uma impossibilidade técnica de ser utilizada, fator esse que pode ser reavaliado conforme a tecnologia vem tornando-se cada vez mais presente em nossa sociedade \cite{impactofictonreiforcingcitizens}.

\section{\textit{Cloud Computing}} \textit{Cloud Computing} ou Computação em Nuvem define uma modalidade de computação que se diferencia pela disponibilização de recursos computacionais, sendo esses recursos infraestrutura, plataforma, \textit{software}, serviços ou armazenamento, de maneira dinâmica e escalável sob demanda \cite{cloudcomputing}. 

Trata-se de um ambiente compartilhado entre vários usuários onde o custo desses recursos segue a lógica de \textit{pay as you go}, ou seja, o usuário é cobrado pelo tempo de utilização de determinado recurso na plataforma de \textit{Cloud}. O fator crucial que permitiu aos provedores de \textit{Cloud} o gerenciamento e segurança desse ambiente é a virtualização, uma vez que torna-se possível o isolamento de aplicações sendo executadas no mesmo \textit{hardware}\cite{cloudcomputing}.

\section{\textit{Big Data}} O termo \textit{Big Data} teve sua primeira aparição como uma tecnologia emergente em 2011 pelo Gartner, em sua análise denominada \textit{Hype Cycle for Emerging Technologies}, que identifica novas tendências no cenário tecnológico mundial, além de reconhecer o ponto em que se encontra \cite{bigdataincontext}.

O \textit{Hype Cycle} do Gartner foi introduzido em 1995 com o intuito de descrever o progresso das tecnologias emergentes no mercado através de uma curva, denominada como \textit{Hype Curve} \cite{understandinghypecicles}.

% Figura
\begin{figure}[!ht]
	\centering	
	\caption[\hspace{0.1cm}Fases do \textit{Hype Cycle}]{Fases do \textit{Hype Cycle}}
	  \vspace{-0.4cm}
	\includegraphics[width=.8\textwidth]{TCC/figuras/explaining_hype_cycle.png}
	% Caption centralizada
% 	\captionsetup{justification=centering}
	% Caption e fonte
	 \vspace{-0.3cm}
	\\\textbf{\footnotesize Fonte: Gartner Research}
	\label{fig:tela1}
\end{figure}

Esse ciclo segue as seguintes fases:

\begin{itemize}
\item \textbf{\textit{Technology Trigger}}: Demonstração de protótipos e pesquisas em laboratório que provocam interesse do público e da indústria;
\item \textbf{\textit{On the Rise}}: Caracterizada pelas expectativas infladas, essa fase é marcada pelo impacto potencial que essa tecnologia terá no mercado e na sociedade como um todo. Primeiras gerações de produtos que demonstram pouca usabilidade, alta especificidade e custos altos de produção/implementação;
\item \textbf{\textit{At the Peak of Inflated Expectations}}: Após a disponibilização da primeira geração dos produtos, a tecnologia é levada aos seus limites conforme a distribuição e o uso são ampliados. As empresas começam a analisar a viabilidade e o benefício de aderir a tecnologia;
\item \textbf{\textit{Sliding Into the Trough of Disillusionment}}: O vale da desilusão ocorre quando a tecnologia falha ao buscar atender as expectativas infladas criadas pela mídia e é desacreditada. Nessa etapa, os produtos são melhorados com os \textit{feedbacks} dos clientes e tenta-se destacar os benefícios de adotar essa tecnologia;
\item \textbf{\textit{Climbing the Slope of Enlightenment}}: Experiências reais e experimentações aumentam em diversos segmentos, levando à um melhor entendimento da tecnologia. Novas gerações de produtos são desenvolvidas e empresas mais flexíveis e tecnologicamente agressivas passam à adotá-la;
\item \textbf{\textit{Entering the Plateau of Productivity}}: Essa fase significa a entrada da tecnologia para a adoção do público \textit{mainstream}, onde os benefícios da tecnologia já são publicamente reconhecidos. Usualmente essas tecnologias são absorvidas em ecossistemas tecnológicos que ampliam sua adoção e sua maturidade; 
\item \textbf{\textit{Post-Plateau}}: Nesta fase a tecnologia já passou por todas as etapas e já se consolidou como parte do cenário tecnológico.
\end{itemize}


% Figura
\begin{figure}[!ht]
	\centering	
	\caption[\hspace{0.1cm}\textit{Big Data} no \textit{Hype Cycle}]{\textit{Big Data} no \textit{Hype Cycle}}
	  \vspace{-0.4cm}
	\includegraphics[width=.8\textwidth]{TCC/figuras/Hype-Cycle-for-emerging-technologies-Source-Gartner-Group-2011.png}
	% Caption centralizada
% 	\captionsetup{justification=centering}
	% Caption e fonte
	 \vspace{-0.3cm}
	\\\textbf{\footnotesize Fonte: Gartner}
	\label{fig:tela1}
\end{figure}

\textit{Big Data} pode ser definido como um cenário onde existe uma volumetria massiva de dados com expressiva heterogeneidade de tal forma que se torna inviável utilizando os métodos e tecnologias tradicionais. Dados estruturados, semi-estruturados e não estruturados precisam ser extraídos e analisados, porém essa complexidade torna impossível o gerenciamento e processamento nos moldes tradicionais. Para este ambiente, temos 3 pontos basilares deste conceito emergente, também comumente conhecidos como os 3 V's de Big Data \cite{bigdatafastdatadatalake}. São eles:
\begin{itemize} 
 \item \textbf{Volume}: Trata-se de uma volumetria absurda de dados; 
 \item \textbf{Variedade}: Dados caracterizados por sua heterogeneidade, sendo eles estruturados, semi-estruturados e não estruturados;
 \item \textbf{Velocidade}: Altíssima velocidade em que os dados são gerados e processados.
\end{itemize}

\section{\textit{Extract, Transform and Load (ETL)}} Um sistema ou processo de \ac{ETL} pode ser considerado como tudo entre a origem do dado e a camada de apresentação do seu sistema analítico. Consiste em três etapas principais:
\begin{itemize}
\item \textbf{\textit{Extract}}: A extração é o primeiro passo de um processo de ETL. É a etapa onde os dados necessários para análise precisam ser extraídos dos sistemas de origem e inseridos no ambiente analítico para as manipulações necessárias;
\item \textbf{\textit{Transform}}: Uma vez que o dado já está no ambiente analítico e pode ser facilmente analisado, inicia-se a etapa de transformação, que consiste em corrigir e normalizar o dado (tipagem, valores nulos, conflitos de domínio), além de relacionar múltiplas fontes de dados para agregar o valor máximo para o negócio;
\item \textbf{\textit{Load}}: A última etapa do processo é caracterizada pela carga ou inserção dos dados processados na etapa de transformação em um ambiente analítico.
\end{itemize}
Ao final deste processo , as bases de dados estarão atualizados e os dados serão disponibilizados para ferramentas de análise e visualização que proverão os resultados para as partes interessadas \cite{thedatawarehousetoolkit}.

\section{\textit{Data Lake}} Se tratando do conceito de \textit{Data Lake}, sua estrutura é mais flexível. O \textit{Data Lake} pode ser definido como um repositório massivo, centralizado e escalável de dados, contendo dados em seu formato de origem, além de possibilitar sua ingestão e processamento neste ambiente \cite{bigdatafastdatadatalake}. 

Uma pecualiaridade do \textit{Data Lake} é a separação do processamento e do armazenamento. Quando comparado  com o \textit{Data Warehouse}, por exemplo, um servidor ou mesmo um cluster de servidores possuem a unidade de processamento e de armazenamento nos mesmos nós. Ou seja, uma instância e responsável pela execução das consultas aplicadas à ele e também do armazenamento dos dados propriamente ditos. Com a ascenção das tecnologias de \textit{Cloud Computing}, temos o barateamento dos serviçõs de \textit{storage}, fator essencial para a construção de um \textit{Data Lake}, onde temos um \textit{storage} escalável e o processamento é realizado à parte, onde utiliza-se de computação distribuída sob demanda \cite{datalakeforenterprises}. 

Geralmente, o \textit{Data Lake} é segmentado em zonas, baseado nas condições e nos objetivos que os dados que cada zona possui. Algumas das zonas são:

\begin{itemize}
 \item \textbf{\textit{Raw Zone}}: A \textit{Raw Zone} é a zona onde os dados são importados. Nesta zona, os dados não sofrem alteração alguma e o seu formato é exatamente igual ao disponibilizado pelo sistema de origem; 
 \item \textbf{\textit{Staged Zone}}: Na \textit{Staged Zone}, os dados já sofrem transformações com o intuito de padronizar os dados. Transformações como de/paras, transformação de \textit{encoding}, formatos de data, entre outras. Operações que tem o único objetivo de manter todos os dados na mesma estrutura para análises posteriores;
 \item \textbf{\textit{Curated Zone}}: Na \textit{Curated Zone}, os dados já estão completamente tratados e prontos para as análises dos usuários de negócio. Inclusive teremos resultados de análises que serão disponibilizados aos usuários e \textit{stackholders}.
 \end{itemize}
 
 Essa separação em zonas possibilita uma flexibilidade na exploração de dados que possibilita aos Cientistas de Dados aplicarem seus modelos de \textit{Machine Learning} nas zonas onde a granularidade dos dados é menor, por exemplo a \textit{Staged Zone}, enquanto os indicadores de negócio podem ser gerados a partir dos dados curados e prontos para análise na \textit{Curated Zone} \cite{datalakeforenterprises}.

\section{Spark} O Apache Spark pode ser definido como uma \textit{engine} computacional de alta velocidade e de propósito geral utilizada para o processamento distribuído de grandes massas de dados. Contém módulos com objetivos específicos, sendo eles \cite{learningspark}:

% Figura
\begin{figure}[!ht]
	\centering	
	\caption[\hspace{0.1cm} Spark]{Arquitetura Spark}
	  \vspace{-0.4cm}
	\includegraphics[width=.8\textwidth]{TCC/figuras/spark.png}
	% Caption centralizada
% 	\captionsetup{justification=centering}
	% Caption e fonte
	 \vspace{-0.3cm}
	\\\textbf{\footnotesize Fonte: Apache}
	\label{fig:tela1}
\end{figure}

\begin{itemize}
 \item \textbf{\textit{Spark Core}}: Spark Core contém toda a estrutura para o funcionamento do Spark. Os componentes para armazenamento, gerenciamento de memória, agendamento de tarefas, aém de definir as o conceito dos \textit{Resilient Distributed Datasets} (RDDs), o conceito basilar da plataforma; 
 \item \textbf{\textit{Spark SQL structured data}}: Spark SQL é o pacote utilizado para prover a manipulação de dados estruturados. Trabalha com diversas fontes de dados incluindo JSON, CSV, SQL, Parquet, entre outras. Possibilita ao usuário utilizar uma sintaxe SQL para manipulação dos dados, além do conceito dos \textit{Dataframes}, que podem ser definidos como um RDD com um schema e funcionalidades similares ao SQL;
 \item \textbf{\textit{Spark Streaming real-time}}: Spark Streaming é o módulo do Apache Spark capaz de processar dados em \textit{streaming}. Ele é capaz de realizar esse processamento através de micro processos em \textit{batch};
 \item \textbf{\textit{MLib machine learning}}: Pacote que fornece diversas APIs aos usuários para trabalhar com métodos estatísticos e algoritmos de \textit{Machine Learning}. Possui funções prontas de algoritmos de classificação, regressão, clusterização, entre outros. Estes métodos se beneficiam da computação distribuída do Spark Core para realizar o processamento;
 \item \textbf{\textit{GraphX graph processing}}: APIs para manipulação de grafos, além de possibilitar a computação distribuídas dessas estruturas de dados, uma vez que utilizam a implementação do Spark Core.
 \end{itemize}
 
O Spark possui recursos para gerenciar de maneira eficiente e escalabidade dos seus processos, distribuindo o processamento de um único nó à milhares de nós. O componente responsável por esse gerenciamento é o \textit{cluster manager}. O Apache Spark suporta três opções de \textit{managers}, sendo eles o Hadoop YARN, o Apache Mesos e o \textit{cluster manager} disponibilizado dentro do próprio Spark denominado Standalone Scheduler \cite{learningspark}.
 
\subsection{\textit{Resilient Distributed Datasets (RDDs)}}
Uma das abstrações basilares dessa plataforma é o conceito dos \textit{Resilient Distributed Datasets} (RDDs), que por sua vez podem ser definidos como coleções imutáveis e distribuídas de objetos. São separados em partições que possibilitam o processamento distribuído através do \textit{cluster}. Os RDDs possuem dois tipos de operações, sendo eles as \textit{transformations} e as \textit{actions}. Suas operações são baseadas em um conceito da computação denominado \textit{Lazy Evaluation}, que significa que as \textit{transformations}, operações que transformam o dado como métodos \textit{map} ou \textit{filter}, não serão executadas no momento em que são invocadas, mas sim empilhadas e organizadas em uma pilha de instruções que serão aplicadas aos dados quando uma \textit{action}, operação de saída, como uma escrita no \textit{shell} ou em um arquivo, for disparada \cite{learningspark}.

Para este trabalho, o processamento foi realizado principalmente utilizando o Apache Spark, logo, a explicação do seu funcionamento se torna pertinente. 

\section{Trabalhos Relacionados}
\label{Trabalhos Relacionados}
Nos últimos anos, houveram inúmeros projetos que tratam e analisam dados
públicos com o intuito de informar a população brasileira, partindo tanto da esfera pública como da própria sociedade. 

Um dos projetos que trouxe grande engajamento por parte da comunidade para a análise de dados políticos foi o projeto Serenata de Amor que, através do uso de \textit{Machine Learning}, analisa os reembolsos realizados pela Câmara dos Deputados e divulgados em sua plataforma de Dados Abertos, identificando atividades incomuns que possam ser caracterizadas como fraude\footnote{Disponível em <https://serenata.ai/> Acesso em: 19 mai, 2020}. 

O Ministério Público de Minas Gerais publicou o Mapa Social. Uma plataforma sumariza os indicadores sociais de diversas instituições públicas afim de informar a população sobre os diversos indicadores do Estado se tratando das temáticas Educação, Segurança e Saúde\footnote{Disponível em <https://mapasocial.mpmg.mp.br/> Acesso em: 19 mai, 2020}. 

O CELUPPI Advogados é um escritório de advocacia brasileiro e criador do Radar Governamental, projeto focado no monitoramento das atividades do Legislativo, utilizando os Dados Abertos da Câmara dos Deputados para realizar parte das suas atividades de monitoramento\footnote{Disponível em <https://radargovernamental.com.br/sobre/> Acesso em: 19 mai, 2020}.
